# -*- coding: utf-8 -*-
"""Batch_ML_processing_pipeline_ml_data_preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o1wg68ifF-joFwx_R92hCNuasiCoT-Kq
"""
import os, glob, re
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.io import fits
from astropy.time import Time
from astropy.wcs import WCS
from astropy.coordinates import SkyCoord
import astropy.units as u
from reproject import reproject_interp
import sep
from astroquery.gaia import Gaia
import imageio.v2 as imageio
import io
from PIL import Image
from pathlib import Path
import sep
from astropy.io import fits
from scipy.ndimage import gaussian_filter
from astropy.io import fits
import matplotlib.pyplot as plt
from astropy.visualization import simple_norm
from math import hypot
"""**Helper functions**"""


def extract_frame_times(solved):
    times = []
    for f in solved:
        hdr = fits.getheader(f)
        if "MJD-OBS" in hdr: times.append(hdr["MJD-OBS"])
        elif "DATE-OBS" in hdr: times.append(Time(hdr["DATE-OBS"]).mjd)
        else: raise ValueError(f"No time keyword in {f}")
    return times

def load_folder_allowlist(csv_path, column=None):
    """
    Read a CSV that lists folder names to process.
    - If `column` is given, use that column.
    - Otherwise: try 'folder', 'Folder', 'name', 'Name'; if none exist, use the first column.
    Returns a set of normalized folder names (stripped).
    """
    df = pd.read_csv(csv_path)
    if column and column in df.columns:
        series = df[column]
    else:
        for cand in ["folder", "Folder", "name", "Name"]:
            if cand in df.columns:
                series = df[cand]
                break
        else:
            # fallback: first column
            series = df.iloc[:, 0]
    allow = {str(x).strip() for x in series.dropna().astype(str)}
    return allow


def background_subtract(aligned, out_dir):
    """
    Background subtraction identical to Mookodi:
    - Uses default mesh parameters (bw,bh,fw,fh)
    - Subtracts full background model (not .back())
    - No alignment smoothing beforehand
    """
    out_dir.mkdir(exist_ok=True)
    bg_files = []
    for f in aligned:
        with fits.open(f) as hdul:
            data = hdul[0].data.astype(np.float64)
            hdr = hdul[0].header

        bkg = sep.Background(data)      # ‚ö†Ô∏è default settings, not tuned
        data_sub = data - bkg           # ‚ö†Ô∏è subtract full background, not bkg.back()

        out = out_dir / f.name.replace(".fits", "_bgsub.fits")
        fits.writeto(out, data_sub, hdr, overwrite=True)
        bg_files.append(out)

    return bg_files


def detect_and_match_gaia_RAW(wcs_solved_files, snr_threshold=1.1):
    """
    Detect on RAW WCS-solved images (no alignment).
    This preserves SNR and matches Mookodi's approach exactly.
    
    Args:
        wcs_solved_files: List of WCS-solved FITS files (wcs_solved/*_wcs.fits)
        snr_threshold: Detection threshold in sigma (default 1.1, Mookodi's default)
    
    Returns:
        frame_detections: List of DataFrames with detections per frame (in RAW coordinates)
        pix_scale: Pixel scale in arcsec/pixel
    """
    frame_detections = []
    ref_file = wcs_solved_files[len(wcs_solved_files)//2]
    
    # Get reference WCS for Gaia query
    with fits.open(ref_file) as hdul:
        hdr = hdul[0].header
        w_ref = WCS(hdr)
        ra_c, dec_c = w_ref.wcs.crval
    
    pix_scale = abs(hdr["CD1_1"]) * 3600
    
    # Gaia query (query once for all frames)
    query = f"""
        SELECT ra, dec, phot_g_mean_mag
        FROM gaiadr2.gaia_source
        WHERE CONTAINS(
        POINT('ICRS', ra, dec),
        BOX('ICRS', {ra_c}, {dec_c}, 0.166667, 0.166667)
        )=1
        """
    print(f"üåü Querying Gaia catalog...")
    stars = Gaia.launch_job_async(query).get_results()
    gaia_coords = SkyCoord(stars["ra"], stars["dec"], unit="deg")
    print(f"‚úÖ Found {len(stars)} Gaia stars in field")
    
    # Process each RAW frame
    for f in wcs_solved_files:
        print(f"\nüìÅ Processing RAW: {f.name}")
        
        # Load RAW WCS-solved data (NOT aligned!)
        with fits.open(f) as hdul:
            data = np.ascontiguousarray(
                np.nan_to_num(hdul[0].data, copy=False).astype(np.float64)
            )
            hdr = hdul[0].header
            w = WCS(hdr)
        
        # Background subtraction (exactly like Mookodi, line 182-184)
        bkg = sep.Background(data, bw=64, bh=64, fw=3, fh=3)
        data_sub = data - bkg.back()  # Note: Mookodi uses bkg.back(), we previously used just 'bkg'
        bkg_rms = bkg.globalrms
        
        print(f"  üìä Background RMS (RAW): {bkg_rms:.2f} counts")
        print(f"  üéØ Detection threshold: {snr_threshold}œÉ = {snr_threshold * bkg_rms:.2f} counts")
        
        # Detection (exactly like Mookodi, line 186)
        radius = 4
        minarea = int((np.pi * radius**2) / 10)
        
        objects = sep.extract(
            data_sub,
            thresh=snr_threshold,  # Use Mookodi's 1.1
            minarea=minarea,
            err=bkg_rms
            # Note: Mookodi doesn't use deblend_cont parameter
        )
        
        n_detections = len(objects) if objects is not None else 0
        print(f"  üîç SEP detections (RAW): {n_detections}")
        
        if objects is None or len(objects) == 0:
            frame_detections.append(
                pd.DataFrame(columns=["x", "y", "flux", "snr", "ra", "dec", "matched_to_gaia"])
            )
            continue
        
        # Calculate SNR
        snr_values = objects["flux"] / (bkg_rms * np.sqrt(objects["npix"]))
        
        # Convert pixel ‚Üí world coordinates (using RAW WCS)
        ras, decs = w.wcs_pix2world(objects["x"], objects["y"], 0)
        
        # Build detection catalog
        df = pd.DataFrame({
            "x": objects["x"],       # RAW pixel coordinates
            "y": objects["y"], 
            "flux": objects["flux"],
            "snr": snr_values,
            "ra": ras,              # World coordinates
            "dec": decs
        })
        
        # Match to Gaia
        det_coords = SkyCoord(df["ra"], df["dec"], unit="deg")
        _, sep2d, _ = det_coords.match_to_catalog_sky(gaia_coords)
        df["matched_to_gaia"] = sep2d.arcsec < 1.5#3.0 gave fewer candidates less star matches
        
        n_gaia = df["matched_to_gaia"].sum()
        n_candidates = len(df) - n_gaia
        print(f"  ‚≠ê {n_gaia} matched to Gaia")
        print(f"  üéØ {n_candidates} asteroid candidates")
        
        if n_candidates > 0:
            candidate_snrs = df[~df["matched_to_gaia"]]["snr"]
            print(f"  üìà Candidate SNR range: {candidate_snrs.min():.1f} - {candidate_snrs.max():.1f}")
        
        frame_detections.append(df)
    
    return frame_detections, pix_scale


def convert_detections_to_aligned_frame(detections, wcs_solved_files, aligned_files):
    """
    Convert detections from RAW coordinates to ALIGNED coordinates.
    
    This allows motion linking on aligned frame while preserving detection quality.
    
    Args:
        detections: List of detection DataFrames (from detect_and_match_gaia_RAW)
        wcs_solved_files: List of RAW WCS-solved files
        aligned_files: List of aligned FITS files
    
    Returns:
        aligned_detections: List of DataFrames with coordinates converted to aligned frame
    """
    aligned_detections = []
    
    for det_df, wcs_file, aligned_file in zip(detections, wcs_solved_files, aligned_files):
        if det_df is None or det_df.empty:
            aligned_detections.append(det_df)
            continue
        
        print(f"  üîÑ Converting {len(det_df)} detections to aligned coordinates...")
        
        # Load WCS from both frames
        with fits.open(wcs_file) as hdul:
            wcs_raw = WCS(hdul[0].header)
        
        with fits.open(aligned_file) as hdul:
            wcs_aligned = WCS(hdul[0].header)
        
        # Convert: RAW pixels ‚Üí RA/Dec ‚Üí ALIGNED pixels
        # We already have RA/Dec in det_df, so just convert to aligned pixels
        x_aligned, y_aligned = wcs_aligned.wcs_world2pix(
            det_df["ra"].values, 
            det_df["dec"].values, 
            0
        )
        
        # Create new dataframe with aligned coordinates
        det_df_aligned = det_df.copy()
        det_df_aligned["x_raw"] = det_df["x"]  # Keep original for reference
        det_df_aligned["y_raw"] = det_df["y"]
        det_df_aligned["x"] = x_aligned       # Replace with aligned coords
        det_df_aligned["y"] = y_aligned
        
        aligned_detections.append(det_df_aligned)
        
        print(f"  ‚úÖ Converted to aligned frame")
    
    return aligned_detections



def save_tracks_to_master(tracks, folder, aligned, times, pix_scale, master_csv):
    """
    Append all motion-linker tracks from a folder into a single master CSV,
    including truth matching, timing, and pixel scale.

    Parameters
    ----------
    tracks : pd.DataFrame
        Motion linker output with f1_x, f2_x, f3_x, etc.
    folder : pathlib.Path
        Folder currently being processed.
    aligned : list
        List of aligned FITS paths (used for DS9 truth reference).
    times : list
        Frame timestamps (MJD values or similar).
    pix_scale : float
        Pixel scale in arcsec/pixel.
    master_csv : str or Path
        Path to the combined master CSV file.
    """

    if tracks is None or tracks.empty:
        print(f"‚ö†Ô∏è No tracks found for {folder.name}, skipping append.")
        return

    # Attach metadata
    tracks["folder"] = folder.name
    tracks["pix_scale"] = pix_scale

    # Compute frame timing
    try:
        dt1_sec = (times[1] - times[0]) * 86400
        dt2_sec = (times[2] - times[1]) * 86400
    except Exception:
        dt1_sec, dt2_sec = np.nan, np.nan

    tracks["dt1_sec"] = dt1_sec
    tracks["dt2_sec"] = dt2_sec


        # --- Reorder columns for readability before saving ---
    preferred_order = [
        "folder", "pix_scale", "dt1_sec", "dt2_sec",
        "f1_x", "f1_y", "f2_x", "f2_y", "f3_x", "f3_y",
        "disp12_px", "disp23_px", "total_disp_px",
        "straightness", "angle_change_deg"
    ]

    # Automatically include any new columns at the end
    cols = [c for c in preferred_order if c in tracks.columns] + [
        c for c in tracks.columns if c not in preferred_order
    ]
    tracks = tracks[cols]

    # Append to master CSV
    tracks.to_csv(
        master_csv,
        mode="a",
        header=not Path(master_csv).exists(),
        index=False
    )
    print(f"üíæ Added {len(tracks)} tracks ‚Üí {master_csv}")


def minutes_between(t2, t1):
    # astropy Time
        if hasattr(t1, "mjd"):
            return float((t2.mjd - t1.mjd) * 1440.0)
        # datetime or pandas Timestamp
        if hasattr(t2, "__sub__") and hasattr((t2 - t1), "total_seconds"):
            return float((t2 - t1).total_seconds() / 60.0)
        # plain floats in days
        return float((t2 - t1) * 1440.0)


def run_pipeline_single(folder, master_csv):
    """
    Process one asteroid folder.
    UPDATED to use catalog-based linking.
    """
    folder = Path(folder)
    # --- 0. Skip any folder with the word "comet" ---
    print(f"\n=== Processing {folder.name} ===")


    # --- Load BOTH raw and aligned files ---
    wcs_solved = sorted((folder / "wcs_solved").glob("*_wcs.fits"))
    aligned = sorted((folder / "aligned").glob("*_wcs_aligned.fits"))
    
    if len(wcs_solved) < 3:
        print(f"‚ö†Ô∏è {folder.name}: missing WCS-solved FITS (found {len(wcs_solved)})")
        return
    if len(aligned) < 3:
        print(f"‚ö†Ô∏è {folder.name}: missing aligned FITS (found {len(aligned)})")
        return

    try:
        times = extract_frame_times(wcs_solved)      # returns MJD values
        dt1 = (times[1] - times[0]) * 86400      # seconds between frame1‚Äìframe2
        dt2 = (times[2] - times[1]) * 86400      # seconds between frame2‚Äìframe3
        print(f"üïí Cadence detected: Œît1={dt1/60:.2f} min, Œît2={dt2/60:.2f} min")
    except Exception as e:
        print(f"‚ö†Ô∏è Cadence extraction failed ({e}), using default 120 s")
        dt1, dt2 = 120.0, 120.0

    print("\nüîç Step 1: Detecting on RAW WCS-solved images...")
    detections_raw, pix_scale = detect_and_match_gaia_RAW(
        wcs_solved, 
        snr_threshold=1.1  # Mookodi's default - no need to lower!
    )

    print("\nüîÑ Step 2: Converting detections to aligned coordinates...")
    detections_aligned = convert_detections_to_aligned_frame(
        detections_raw, 
        wcs_solved, 
        aligned
    )
    
    detections = detections_aligned
    
    print(f"\n   üîç Per-frame detection summary:")
    for i, det in enumerate(detections):
        if det is not None and not det.empty:
            n_total = len(det)
            n_gaia = det["matched_to_gaia"].sum() if "matched_to_gaia" in det.columns else 0
            n_asteroid_candidates = n_total - n_gaia
            print(f"      Frame {i+1}: {n_total} detections ({n_gaia} stars, {n_asteroid_candidates} candidates)")

    # ============================================================
    # 3.4 SAVE SEP + GAIA DETECTIONS TO CSV
    output_rows = []

    for frame_id, det_df in enumerate(detections, start=1):
        if det_df is None or len(det_df) == 0:
            continue
        for _, d in det_df.iterrows():
            output_rows.append({
                "frame": frame_id,
                "x": d.get("x", np.nan),
                "y": d.get("y", np.nan),
                "flux": d.get("flux", np.nan),
                "snr": d.get("snr", np.nan),
                "matched_to_gaia": bool(d.get("matched_to_gaia", False))
            })

    if output_rows:
        df_out = pd.DataFrame(output_rows)
        out_path = Path(folder) / "detections_with_gaia_aligned.csv"
        df_out.to_csv(out_path, index=False)
        print(f"‚úÖ Saved SEP + Gaia detections to: {out_path}")
    else:
        print(f"‚ö†Ô∏è No detections to save for {folder.name}")

    # ============================================================
    # 3B. REMOVE GAIA STARS, SHOW PLOT, AND PREPARE DETECTIONS FOR LINKER
    # ============================================================

    print("\nüî≠ Processing Gaia-filtered detections (no reprojection)...")

    # Make output folder for debug plots
    gaia_debug_dir = folder / "gaia_filter_debug"
    gaia_debug_dir.mkdir(exist_ok=True, parents=True)

    updated_detections = []
    gaia_lists = []  # <-- NEW: keep Gaia-only detections per frame for alignment diagnostics
    for i, (df, bg_file) in enumerate(zip(detections, aligned)):
        if df is None or df.empty:
            print(f"‚ö†Ô∏è Frame {i+1}: No detections found.")
            updated_detections.append(pd.DataFrame(columns=["x", "y", "flux"]))
            continue

        # Split Gaia vs Non-Gaia
        gaia_df = df[df["matched_to_gaia"]].copy()
        gaia_lists.append(gaia_df[["x","y"]].copy())  # <-- NEW: store Gaia-star pixels per frame
        non_gaia_df = df[~df["matched_to_gaia"]].copy()
        print(f"üì∏ Frame {i+1}: {len(df)} detections ‚Üí "
            f"{len(gaia_df)} Gaia, {len(non_gaia_df)} non-Gaia")

        # ---- PLOT Gaia vs Non-Gaia detections on BACKGROUND-SUBTRACTED IMAGE ----
        try:
            with fits.open(bg_file) as h:
                img = h[0].data.astype(float)
            fig, ax = plt.subplots(figsize=(8, 8))
            norm = simple_norm(img, "asinh", percent=99)
            ax.imshow(img, cmap="gray", origin="lower", norm=norm)

            if len(gaia_df):
                ax.scatter(gaia_df["x"], gaia_df["y"], s=15, color="cyan", label="Gaia stars", alpha=0.6)
            if len(non_gaia_df):
                ax.scatter(non_gaia_df["x"], non_gaia_df["y"], s=20, color="lime", label="Remaining candidates", alpha=0.7)

            ax.legend()
            ax.set_title(f"Frame {i+1}: Gaia vs Non-Gaia Detections (bg-sub)")
            plt.savefig(gaia_debug_dir / f"gaia_filter_frame{i+1}.png", dpi=150)
            plt.close(fig)
        except Exception as e:
            print(f"‚ö†Ô∏è Plotting failed for frame {i+1}: {e}")

        # Add to detections list for linker
        updated_detections.append(non_gaia_df)

    detections = updated_detections
    print(f"‚úÖ Gaia-filtered detections ready for motion linking "
        f"({sum(len(df) for df in detections if df is not None)} total candidates).")

    # ============================================================
    # üî¨ DIAGNOSTICS ‚Äî Gaia alignment only (truth-free)
    #   Computes œÉ_align_px from Gaia star‚Äìstar nearest-neighbor residuals.
    # ============================================================

    try:
        from scipy.spatial import cKDTree as KDTree
        _HAVE_KD = True
    except Exception:
        KDTree = None
        _HAVE_KD = False

    def _gaia_alignment_residuals(dfA, dfB, max_pair_px=8.0):
        """
        Star‚Äìstar alignment residuals using Gaia-matched detections (truth-free).
        For each star in A, take NN in B within max_pair_px and summarize distances.
        """
        import numpy as np
        if dfA is None or dfB is None or dfA.empty or dfB.empty:
            return dict(n=0, median_px=np.nan, p90_px=np.nan, mean_px=np.nan)

        A = dfA[["x","y"]].to_numpy(float)
        B = dfB[["x","y"]].to_numpy(float)

        res = []
        if _HAVE_KD:
            tree = KDTree(B)
            for p in A:
                d, _ = tree.query(p, k=1)
                if d <= max_pair_px:
                    res.append(float(d))
        else:
            for p in A:
                d = float(np.min(np.hypot(B[:,0]-p[0], B[:,1]-p[1])))
                if d <= max_pair_px:
                    res.append(d)

        if not res:
            return dict(n=0, median_px=np.nan, p90_px=np.nan, mean_px=np.nan)

        arr = np.array(res, dtype=float)
        return dict(
            n=len(arr),
            median_px=float(np.median(arr)),
            p90_px=float(np.quantile(arr, 0.90)),
            mean_px=float(np.mean(arr)),
        )

    # Compute once; use a single, robust fallback
    try:
        r12 = _gaia_alignment_residuals(gaia_lists[0], gaia_lists[1])
        r23 = _gaia_alignment_residuals(gaia_lists[1], gaia_lists[2])
        print("üìê STAR‚ÄìSTAR ALIGNMENT (Gaia NN residuals)")
        print(f"  F1‚ÜíF2: n={r12['n']}, median={r12['median_px']:.2f}px, p90={r12['p90_px']:.2f}px")
        print(f"  F2‚ÜíF3: n={r23['n']}, median={r23['median_px']:.2f}px, p90={r23['p90_px']:.2f}px")

        # Floor of 0.5 px; if both medians are NaN, fall back to 1.5 px
        med12 = r12.get('median_px', np.nan)
        med23 = r23.get('median_px', np.nan)
        sigma_align_guess = np.nanmax([med12, med23, 1.5])
        sigma_align_guess = max(0.5, float(sigma_align_guess))
    except Exception as _e:
        print(f"üìê STAR‚ÄìSTAR ALIGNMENT skipped ({_e})")
        sigma_align_guess = 1.5

    print(f"üìè Using sigma_align_px ‚âà {sigma_align_guess:.2f} px for prediction radius.")


    # ============================================================
    # 4. MOTION LINKING ON FILTERED DETECTIONS (GEOMETRY-ONLY, 3 FRAMES)
    # ============================================================
    from Core_pipeline.motion_linker_ppa3_v3_5 import PPA3LinkerV2, LinkParamsV2

    # Hygiene: drop non-finite x/y before building candidate lists
    def _clean(df):
        if df is None: return None
        m = np.isfinite(df["x"]) & np.isfinite(df["y"])
        return df.loc[m].copy()

    detections = [ _clean(d) for d in detections ]

    # Build candidate dicts the linker expects (x,y[,sigma,id])
    def _to_cands(df):
        if df is None or df.empty: return []
        return [{'x': float(r.x), 'y': float(r.y), 'sigma': 1.0, 'id': int(i)}
                for i, r in df.reset_index(drop=True).iterrows()]

    C1 = _to_cands(detections[0])
    C2 = _to_cands(detections[1])
    C3 = _to_cands(detections[2])

    if not (len(C1) and len(C2) and len(C3)):
        print("‚ö†Ô∏è One or more frames have zero candidates; skipping linker.")
        tracks = pd.DataFrame(columns=["f1_x","f1_y","f2_x","f2_y","f3_x","f3_y",
                                    "score","residual_px","cross_track_px",
                                    "angle_change_deg","vel_ratio","r_pred_px","total_disp_px"])
    else:
        # Times ‚Üí seconds relative to t1 (constant-velocity uses dt ratios; seconds is robust)
        t1s, t2s, t3s = [(times[i]-times[0])*86400.0 for i in range(3)]

        params = LinkParamsV2(
            sigma_align_px=float(sigma_align_guess),

        )

        linker = PPA3LinkerV2(params=params)
        linked = linker.link_three_frames(C1, C2, C3, t1s, t2s, t3s)

        rows = []
        for tr in linked:
            rows.append({
                "f1_x": tr["p1"]["x"], "f1_y": tr["p1"]["y"],
                "f2_x": tr["p2"]["x"], "f2_y": tr["p2"]["y"],
                "f3_x": tr["p3"]["x"], "f3_y": tr["p3"]["y"],
                "score": tr["J"],
                "residual_px": tr["residual_px"],
                "cross_track_px": tr["cross_track_px"],
                "angle_change_deg": tr["angle_change_deg"],
                "vel_ratio": tr["vel_ratio"],
                "r_pred_px": tr["r_pred_px"],
                "total_disp_px": tr["total_disp_px"],
                "disp12_px": tr["disp12_px"],    # ‚úÖ ADD THIS
                "disp23_px": tr["disp23_px"],    # ‚úÖ ADD THIS
            })
        tracks = pd.DataFrame(rows)

    print(f"‚úÖ Motion linker (v2) produced {len(tracks)} three-frame track(s).")
 
    # ============================================================
    # 5. FEATURE EXTRACTION + MOTION SCORING
    # ============================================================
    expected_cols = {"f1_x", "f1_y", "f2_x", "f2_y", "f3_x", "f3_y"}
    if not expected_cols.issubset(tracks.columns):
        print("‚ùå Track format error - missing coordinate columns")
        return

    # Compatibility tuples
    for f in ["f1", "f2", "f3"]:
        tracks[f] = list(zip(tracks[f"{f}_x"], tracks[f"{f}_y"]))

    print(f"üßÆ Tracks summary: {len(tracks)} total")

    # Time deltas (if times available)
    dt1_min = minutes_between(times[1], times[0])
    dt2_min = minutes_between(times[2], times[1])

    # Feature computation
    save_tracks_to_master(
    tracks=tracks,
    folder=folder,
    aligned=aligned,
    times=times,
    pix_scale=pix_scale,
    master_csv=master_csv
)



def safe_run_pipeline_single(subfolder, master_csv, error_log="skipped_folders.txt"):
    """
    Wrap run_pipeline_single with folder validation + error logging.
    """
    try:
        run_pipeline_single(subfolder, master_csv)
    except Exception as e:
        with open(error_log, "a") as f:
            f.write(f"{subfolder}: {str(e)}\n")
        print(f"‚ö†Ô∏è Skipped {subfolder.name} due to error: {e}")


def run_pipeline_all(root="cleaned_data",
                     master_csv="all_motion_candidates.csv",
                     start=0, end=None,
                     error_log="skipped_folders.txt",
                     include_csv=None,          # <--- NEW: path to CSV with folder names
                     include_csv_column=None):  # <--- NEW: optional column name
    """
    Run the pipeline on folders under `root`, append to master CSV,
    optionally restricting to folders listed in `include_csv`.
    Logs any skipped/errored folders.
    """
    Path(master_csv).unlink(missing_ok=True)
    Path(error_log).unlink(missing_ok=True)

    # Gather all subfolders under root
    all_folders = [f for f in sorted(Path(root).iterdir()) if f.is_dir()]

    # If an allowlist CSV is provided, filter to those names
    if include_csv is not None:
        allow = load_folder_allowlist(include_csv, column=include_csv_column)

        # map of {folder_name -> Path} for fast lookup and to keep root‚Äôs sort order
        name_to_path = {f.name: f for f in all_folders}
        missing = sorted([name for name in allow if name not in name_to_path])

        # only keep folders that are both in root and in the CSV
        folders = [name_to_path[name] for name in name_to_path.keys() if name in allow]
        #folders = folders[0:2]

        # Log any names in CSV that don‚Äôt exist under root
        if missing:
            with open(error_log, "a") as f:
                for m in missing:
                    f.write(f"{m}: listed in CSV but not found under root\n")
            print(f"‚ö†Ô∏è {len(missing)} folder(s) listed in CSV not found under root. Logged in {error_log}.")
    else:
        folders = all_folders

    # Slice (start/end) after filtering so indices match the filtered set
    if end is None:
        end = len(folders)
    folders = folders[start:end]

    print(f"üìÇ Processing {len(folders)} folders (index {start} to {end-1})")
    for i, sub in enumerate(folders, start=start):
        print(f"\n[{i}] --- {sub.name} ---")
        safe_run_pipeline_single(sub, master_csv, error_log=error_log)

    print(f"\n‚úÖ Master CSV saved at {master_csv}")
    print(f"üìù Skipped/invalid folders logged in {error_log}")