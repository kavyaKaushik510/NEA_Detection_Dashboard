{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13498608,"sourceType":"datasetVersion","datasetId":8570624},{"sourceId":13498669,"sourceType":"datasetVersion","datasetId":8570675},{"sourceId":13535133,"sourceType":"datasetVersion","datasetId":8594822}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nDataset Diagnostics - Understand Your Data Before Training\n===========================================================\nThis script analyzes your preprocessed stamps to:\n1. Verify normalization is correct\n2. Check for data quality issues\n3. Visualize sample stamps\n4. Analyze class balance and sources\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Config\nDATA_DIR = Path(\"/kaggle/input/stamps-clean/stamps_clean\")\nMETADATA_CSV = DATA_DIR / \"metadata.csv\"\nWORK_DIR = Path(\"/kaggle/working\")\nN_SAMPLES = 10  # samples to visualize per class\n\ndef analyze_normalization(metadata, data_dir, n_check=100):\n    \"\"\"\n    Check if data is properly normalized.\n    Should see mean~0, std~1, values in [-5, 5] range.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"NORMALIZATION CHECK\")\n    print(\"=\"*60)\n    \n    # Sample random stamps\n    sample_paths = metadata.sample(min(n_check, len(metadata)))['stamp_path'].tolist()\n    \n    all_values = []\n    for path in tqdm(sample_paths, desc=\"Loading samples\"):\n        arr = np.load(data_dir / path)['x']\n        all_values.append(arr.flatten())\n    \n    all_values = np.concatenate(all_values)\n    \n    print(f\"\\nStatistics across {n_check} random stamps:\")\n    print(f\"  Mean:   {all_values.mean():.4f} (expect: ~0)\")\n    print(f\"  Std:    {all_values.std():.4f} (expect: ~1-2)\")\n    print(f\"  Min:    {all_values.min():.4f} (expect: >=-5)\")\n    print(f\"  Max:    {all_values.max():.4f} (expect: <=5)\")\n    print(f\"  Median: {np.median(all_values):.4f}\")\n    \n    # Check for NaN/Inf\n    n_nan = np.isnan(all_values).sum()\n    n_inf = np.isinf(all_values).sum()\n    print(f\"\\n  NaN count: {n_nan}\")\n    print(f\"  Inf count: {n_inf}\")\n    \n    if n_nan > 0 or n_inf > 0:\n        print(\"  ⚠️  WARNING: Found NaN or Inf values!\")\n    \n    # Distribution plot\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(all_values, bins=100, alpha=0.7, edgecolor='black')\n    plt.xlabel('Pixel Value')\n    plt.ylabel('Count')\n    plt.title('Pixel Value Distribution (All Channels)')\n    plt.axvline(x=0, color='r', linestyle='--', label='Mean=0')\n    plt.axvline(x=-5, color='orange', linestyle='--', label='Clip bounds')\n    plt.axvline(x=5, color='orange', linestyle='--')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    plt.hist(all_values, bins=100, alpha=0.7, edgecolor='black', cumulative=True, density=True)\n    plt.xlabel('Pixel Value')\n    plt.ylabel('Cumulative Probability')\n    plt.title('Cumulative Distribution')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(WORK_DIR / 'normalization_check.png', dpi=150)\n    print(f\"\\n✓ Saved normalization plot: {WORK_DIR / 'normalization_check.png'}\")\n    plt.close()\n\n\ndef analyze_class_balance(metadata):\n    \"\"\"Analyze class distribution across splits and sources\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"CLASS BALANCE ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Overall balance\n    print(\"\\n--- Overall ---\")\n    print(metadata.groupby('label').size())\n    total_pos = (metadata['label'] == 1).sum()\n    total_neg = (metadata['label'] == 0).sum()\n    print(f\"Imbalance ratio: 1:{total_neg/total_pos:.1f}\")\n    \n    # By split\n    print(\"\\n--- By Split ---\")\n    split_balance = metadata.groupby(['split', 'label']).size().unstack(fill_value=0)\n    print(split_balance)\n    \n    for split in ['train', 'val', 'test']:\n        pos = split_balance.loc[split, 1] if 1 in split_balance.columns else 0\n        neg = split_balance.loc[split, 0] if 0 in split_balance.columns else 0\n        if pos > 0:\n            print(f\"  {split}: 1:{neg/pos:.1f}\")\n    \n    # By source (negatives only)\n    print(\"\\n--- Negative Sources ---\")\n    neg_sources = metadata[metadata['label'] == 0].groupby('source').size()\n    print(neg_sources)\n    print(f\"\\nTotal negative sources: {len(neg_sources)}\")\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Split balance\n    split_balance.plot(kind='bar', ax=axes[0], color=['blue', 'red'])\n    axes[0].set_title('Class Balance by Split')\n    axes[0].set_xlabel('Split')\n    axes[0].set_ylabel('Count')\n    axes[0].legend(['Negative', 'Positive'])\n    axes[0].grid(True, alpha=0.3)\n    \n    # Source distribution (negatives)\n    neg_sources.plot(kind='bar', ax=axes[1], color='steelblue')\n    axes[1].set_title('Negative Sample Sources')\n    axes[1].set_xlabel('Source Type')\n    axes[1].set_ylabel('Count')\n    axes[1].tick_params(axis='x', rotation=45)\n    axes[1].grid(True, alpha=0.3)\n    \n    # Pie chart\n    axes[2].pie([total_neg, total_pos], labels=['Negative', 'Positive'], \n                autopct='%1.1f%%', colors=['blue', 'red'], startangle=90)\n    axes[2].set_title('Overall Class Distribution')\n    \n    plt.tight_layout()\n    plt.savefig(WORK_DIR / 'class_balance.png', dpi=150)\n    print(f\"\\n✓ Saved class balance plot: {WORK_DIR / 'class_balance.png'}\")\n    plt.close()\n\n\ndef visualize_samples(metadata, data_dir, n_samples=10):\n    \"\"\"Visualize sample stamps for each class\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"VISUALIZING SAMPLE STAMPS\")\n    print(\"=\"*60)\n    \n    for label in [0, 1]:\n        label_name = \"POSITIVE (Asteroid)\" if label == 1 else \"NEGATIVE (Non-asteroid)\"\n        samples = metadata[metadata['label'] == label].sample(min(n_samples, (metadata['label'] == label).sum()))\n        \n        fig, axes = plt.subplots(n_samples, 5, figsize=(15, 3*n_samples))\n        if n_samples == 1:\n            axes = axes.reshape(1, -1)\n        \n        fig.suptitle(f'{label_name} Samples', fontsize=16, y=0.995)\n        \n        for i, (idx, row) in enumerate(samples.iterrows()):\n            arr = np.load(data_dir / row['stamp_path'])['x']\n            \n            # Handle shape\n            if arr.ndim == 3 and arr.shape[-1] == 5:\n                arr = np.transpose(arr, (2, 0, 1))\n            \n            # Display each channel\n            channel_names = ['F1', 'F2', 'F3', 'S2', 'S3']\n            for j in range(5):\n                ax = axes[i, j]\n                im = ax.imshow(arr[j], cmap='gray', vmin=-3, vmax=3)\n                ax.set_title(f\"{channel_names[j]}\\n{row['source']}\", fontsize=8)\n                ax.axis('off')\n                \n                if j == 4:  # Add colorbar to last channel\n                    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n        \n        plt.tight_layout()\n        save_name = f'samples_{\"positive\" if label == 1 else \"negative\"}.png'\n        plt.savefig(WORK_DIR / save_name, dpi=150, bbox_inches='tight')\n        print(f\"✓ Saved {label_name} samples: {WORK_DIR / save_name}\")\n        plt.close()\n\n\ndef analyze_spatial_distribution(metadata):\n    \"\"\"Analyze spatial distribution of samples across folders\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"SPATIAL DISTRIBUTION ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Samples per folder\n    folder_counts = metadata.groupby('folder').size()\n    print(f\"\\nTotal folders: {len(folder_counts)}\")\n    print(f\"Mean samples per folder: {folder_counts.mean():.1f}\")\n    print(f\"Median samples per folder: {folder_counts.median():.1f}\")\n    print(f\"Min samples per folder: {folder_counts.min()}\")\n    print(f\"Max samples per folder: {folder_counts.max()}\")\n    \n    # Positives per folder (should be ~1)\n    pos_per_folder = metadata[metadata['label'] == 1].groupby('folder').size()\n    print(f\"\\nFolders with positives: {len(pos_per_folder)}\")\n    print(f\"Mean positives per folder: {pos_per_folder.mean():.2f}\")\n    \n    # Histogram\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(folder_counts, bins=30, edgecolor='black', alpha=0.7)\n    plt.xlabel('Samples per Folder')\n    plt.ylabel('Count')\n    plt.title('Distribution of Samples Across Folders')\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    split_folder_counts = metadata.groupby(['split', 'folder']).size().unstack(fill_value=0)\n    split_folder_counts.sum().plot(kind='bar', color='steelblue')\n    plt.xlabel('Split')\n    plt.ylabel('Total Folders')\n    plt.title('Number of Folders per Split')\n    plt.xticks(rotation=0)\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(WORK_DIR / 'spatial_distribution.png', dpi=150)\n    print(f\"\\n✓ Saved spatial distribution plot: {WORK_DIR / 'spatial_distribution.png'}\")\n    plt.close()\n\n\ndef analyze_channel_statistics(metadata, data_dir, n_samples=50):\n    \"\"\"Analyze per-channel statistics\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PER-CHANNEL STATISTICS\")\n    print(\"=\"*60)\n    \n    # Sample stamps\n    samples = metadata.sample(min(n_samples, len(metadata)))\n    \n    channel_stats = {f'ch{i}': {'mean': [], 'std': [], 'min': [], 'max': []} \n                     for i in range(5)}\n    \n    for _, row in tqdm(samples.iterrows(), total=len(samples), desc=\"Analyzing channels\"):\n        arr = np.load(data_dir / row['stamp_path'])['x']\n        if arr.ndim == 3 and arr.shape[-1] == 5:\n            arr = np.transpose(arr, (2, 0, 1))\n        \n        for i in range(5):\n            channel_stats[f'ch{i}']['mean'].append(arr[i].mean())\n            channel_stats[f'ch{i}']['std'].append(arr[i].std())\n            channel_stats[f'ch{i}']['min'].append(arr[i].min())\n            channel_stats[f'ch{i}']['max'].append(arr[i].max())\n    \n    # Print summary\n    channel_names = ['F1 (Frame 1)', 'F2 (Frame 2)', 'F3 (Frame 3)', \n                     'S2 (F2-med)', 'S3 (F3-med)']\n    \n    for i, name in enumerate(channel_names):\n        ch_key = f'ch{i}'\n        print(f\"\\n{name}:\")\n        print(f\"  Mean:  {np.mean(channel_stats[ch_key]['mean']):.3f} ± {np.std(channel_stats[ch_key]['mean']):.3f}\")\n        print(f\"  Std:   {np.mean(channel_stats[ch_key]['std']):.3f} ± {np.std(channel_stats[ch_key]['std']):.3f}\")\n        print(f\"  Range: [{np.mean(channel_stats[ch_key]['min']):.3f}, {np.mean(channel_stats[ch_key]['max']):.3f}]\")\n    \n    # Box plots\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    for idx, stat_name in enumerate(['mean', 'std', 'min', 'max']):\n        ax = axes[idx // 2, idx % 2]\n        data = [channel_stats[f'ch{i}'][stat_name] for i in range(5)]\n        ax.boxplot(data, labels=channel_names)\n        ax.set_title(f'Channel {stat_name.capitalize()} Distribution')\n        ax.set_ylabel(stat_name.capitalize())\n        ax.grid(True, alpha=0.3)\n        ax.tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig(WORK_DIR / 'channel_statistics.png', dpi=150)\n    print(f\"\\n✓ Saved channel statistics plot: {WORK_DIR / 'channel_statistics.png'}\")\n    plt.close()\n\n\ndef main():\n    print(\"=\"*60)\n    print(\"DATASET DIAGNOSTICS\")\n    print(\"=\"*60)\n    \n    # Load metadata\n    if not METADATA_CSV.exists():\n        print(f\"❌ Metadata not found at {METADATA_CSV}\")\n        return\n    \n    metadata = pd.read_csv(METADATA_CSV)\n    print(f\"✓ Loaded metadata: {len(metadata)} samples\")\n    \n    WORK_DIR.mkdir(exist_ok=True, parents=True)\n    \n    # Run analyses\n    analyze_class_balance(metadata)\n    analyze_spatial_distribution(metadata)\n    analyze_normalization(metadata, DATA_DIR, n_check=100)\n    analyze_channel_statistics(metadata, DATA_DIR, n_samples=50)\n    visualize_samples(metadata, DATA_DIR, n_samples=N_SAMPLES)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"DIAGNOSTIC COMPLETE\")\n    print(\"=\"*60)\n    print(\"\\nGenerated files:\")\n    print(f\"  - {WORK_DIR / 'class_balance.png'}\")\n    print(f\"  - {WORK_DIR / 'spatial_distribution.png'}\")\n    print(f\"  - {WORK_DIR / 'normalization_check.png'}\")\n    print(f\"  - {WORK_DIR / 'channel_statistics.png'}\")\n    print(f\"  - {WORK_DIR / 'samples_positive.png'}\")\n    print(f\"  - {WORK_DIR / 'samples_negative.png'}\")\n    \n    print(\"\\n💡 Review these diagnostics before training!\")\n    print(\"   Key things to check:\")\n    print(\"   1. Normalization: mean~0, std~1-2, range [-5,5]\")\n    print(\"   2. No NaN/Inf values\")\n    print(\"   3. Class balance matches expectations (1:20)\")\n    print(\"   4. Sample stamps look correct (asteroid visible in F2/F3, difference in S2/S3)\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta.groupby([\"split\",\"label\"]).size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/stamps-clean/stamps_clean\")\nmeta = pd.read_csv(DATA_DIR / \"metadata.csv\")\n\nbad_files = []\nfor i, row in meta.iterrows():\n    path = DATA_DIR / row[\"stamp_path\"]\n    arr = np.load(path)[\"x\"]\n    if not np.isfinite(arr).all():  # checks NaN and Inf\n        bad_files.append(path.name)\n\nprint(f\"Found {len(bad_files)} bad files out of {len(meta)} total.\")\nprint(\"Example bad files:\", bad_files[:5])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nnan_total = 0\nnan_pos = 0\nnan_neg = 0\n\nfor _, row in meta.iterrows():\n    arr = np.load(DATA_DIR / row['stamp_path'])['x']\n    if not np.isfinite(arr).all():\n        nan_total += 1\n        if row[\"label\"] == 1:\n            nan_pos += 1\n        else:\n            nan_neg += 1\n\nprint(f\"Files with NaN: {nan_total}/{len(meta)}\")\nprint(f\"  ↳ Positives with NaN: {nan_pos}\")\nprint(f\"  ↳ Negatives with NaN: {nan_neg}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nImproved 5-Channel Asteroid Detection Training\n================================================\nResearch-backed improvements:\n1. Fixed normalization (removed double normalization bug)\n2. Focal Loss for extreme class imbalance (1:20)\n3. Proper data augmentation (geometric only, no photometry changes)\n4. Learning rate scheduling with warmup\n5. Early stopping on recall @ high precision\n6. Improved initialization for 5-channel input\n\nBased on research from:\n- ATLAS two-stage CNN (Rabeendran et al. 2021)\n- ZTF DeepStreaks (Wang et al. 2022)\n- NEA detection papers in project knowledge\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom pathlib import Path\nfrom sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ===================== CONFIG =====================\nDATA_DIR = Path(\"/kaggle/input/stamps-clean/stamps_clean\")\nWORK_DIR = Path(\"/kaggle/working\")\nACTIVE_CSV = DATA_DIR / \"metadata.csv\"\n\n# Training hyperparameters\nBATCH_SIZE = 64\nNUM_EPOCHS = 50\nBASE_LR = 1e-3\nWEIGHT_DECAY = 1e-4\nWARMUP_EPOCHS = 3\n\n# Focal Loss parameters (research-recommended)\nFOCAL_ALPHA = 0.25  # weight for positive class\nFOCAL_GAMMA = 2.0   # focusing parameter (higher = more focus on hard examples)\n\n# Early stopping (prioritize recall)\nPATIENCE = 7\nMIN_PRECISION = 0.80  # don't sacrifice too much precision\n\n# Augmentation\nAUG_PROB = 0.5\n\n# Random seed\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# ===================== FOCAL LOSS =====================\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for addressing extreme class imbalance.\n    \n    From: Lin et al. \"Focal Loss for Dense Object Detection\" (2017)\n    Used in: ATLAS CNN (Rabeendran 2021), multiple asteroid detection papers\n    \n    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n    \n    where:\n    - p_t is the model's estimated probability for the correct class\n    - alpha_t balances positive/negative importance\n    - gamma focuses on hard examples (gamma=0 -> standard CE)\n    \n    For 1:20 imbalance with high recall priority:\n    - alpha=0.25 gives positive class 4x weight (compensates for 20:1 ratio partially)\n    - gamma=2.0 down-weights easy negatives exponentially\n    \"\"\"\n    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs: (N,) predicted logits\n            targets: (N,) true labels (0 or 1)\n        \"\"\"\n        # Convert to probabilities\n        p = torch.sigmoid(inputs)\n        \n        # Compute focal loss components\n        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        p_t = p * targets + (1 - p) * (1 - targets)  # p if y=1, 1-p if y=0\n        focal_weight = (1 - p_t) ** self.gamma\n        \n        # Alpha weighting (give more weight to positive class)\n        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n        \n        focal_loss = alpha_t * focal_weight * ce_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n\n# ===================== DATASET =====================\nclass AsteroidDataset(Dataset):\n    \"\"\"\n    Dataset for 5-channel asteroid stamps.\n    \n    CRITICAL FIX: Data is already normalized during prep with MAD-based robust z-score.\n    We do NOT re-normalize here (this was causing double normalization bug).\n    \n    Channels: [F1, F2, F3, S2, S3] where:\n    - F1, F2, F3: temporal frames\n    - S2 = F2 - median(F1, F3): difference image highlighting F2\n    - S3 = F3 - median(F1, F2): difference image highlighting F3\n    \n    Each channel already normalized: (x - median) / (1.4826 * MAD), clipped to [-5, 5]\n    \"\"\"\n    def __init__(self, csv_path, split, base_root, augment=False):\n        meta = pd.read_csv(csv_path)\n        self.df = meta[meta[\"split\"] == split].reset_index(drop=True)\n        self.base_root = Path(base_root)\n                # In AsteroidDataset.__init__, after loading df:\n        print(f\"  Loading {split}: filtering NaN files...\")\n        valid_idx = []\n        for idx, row in self.df.iterrows():\n            arr = np.load(self.base_root / row[\"stamp_path\"])[\"x\"]\n            if np.isfinite(arr).all():\n                valid_idx.append(idx)\n        self.df = self.df.iloc[valid_idx].reset_index(drop=True)\n        self.augment = augment\n        self.split = split\n\n        \n        \n        # Track class distribution\n        pos_count = (self.df[\"label\"] == 1).sum()\n        neg_count = (self.df[\"label\"] == 0).sum()\n        print(f\"{split.upper()}: {pos_count} positives, {neg_count} negatives \"\n              f\"(ratio 1:{neg_count/max(pos_count,1):.1f})\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load preprocessed stamp (already normalized!)\n        arr = np.load(self.base_root / row[\"stamp_path\"])[\"x\"].astype(np.float32)\n        \n        # Ensure channel-first format (C, H, W)\n        if arr.ndim == 3 and arr.shape[-1] == 5:\n            arr = np.transpose(arr, (2, 0, 1))\n        \n        # Handle any NaN/inf (should be rare after preprocessing)\n        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n        \n        # GEOMETRIC AUGMENTATION ONLY (no photometry changes!)\n        # Research shows: preserve photometric properties for asteroid detection\n        if self.augment and self.split == \"train\":\n            arr = self._augment(arr)\n        \n        x = torch.tensor(arr, dtype=torch.float32)\n        y = torch.tensor([float(row[\"label\"])], dtype=torch.float32)\n        \n        return x, y\n    \n    def _augment(self, arr):\n        \"\"\"\n        Geometric augmentations only - NO brightness/contrast changes.\n        \n        From research: Photometric changes corrupt magnitude information.\n        Safe augmentations: rotations, flips (preserve pixel values)\n        \"\"\"\n        if np.random.rand() < AUG_PROB:\n            # Random 90-degree rotation\n            k = np.random.randint(0, 4)\n            if k > 0:\n                arr = np.rot90(arr, k=k, axes=(1, 2)).copy()\n        \n        if np.random.rand() < AUG_PROB:\n            # Horizontal flip\n            arr = np.flip(arr, axis=2).copy()\n        \n        if np.random.rand() < AUG_PROB:\n            # Vertical flip  \n            arr = np.flip(arr, axis=1).copy()\n        \n        return arr\n\n\n# ===================== MODEL =====================\nclass EfficientNet5Channel(nn.Module):\n    \"\"\"\n    EfficientNet-B0 adapted for 5-channel input.\n    \n    IMPROVED INITIALIZATION:\n    Instead of just averaging RGB and duplicating, we:\n    1. Use all 3 RGB channels for temporal channels (F1, F2, F3)\n    2. Initialize difference channels (S2, S3) with Xavier/He initialization\n    \n    This gives the model better starting points for learning temporal vs difference features.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        # Load pretrained EfficientNet-B0\n        try:\n            base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n        except:\n            base = models.efficientnet_b0(pretrained=True)\n        \n        # ===== IMPROVED 5-CHANNEL INITIALIZATION =====\n        old_conv = base.features[0][0]\n        new_conv = nn.Conv2d(\n            in_channels=5,\n            out_channels=old_conv.out_channels,\n            kernel_size=old_conv.kernel_size,\n            stride=old_conv.stride,\n            padding=old_conv.padding,\n            bias=False\n        )\n        \n        with torch.no_grad():\n            # Get pretrained RGB weights\n            rgb_weights = old_conv.weight.data.clone()  # (out_ch, 3, k, k)\n            \n            # Initialize new 5-channel weights\n            new_weights = torch.zeros(\n                old_conv.out_channels, 5, \n                old_conv.kernel_size[0], old_conv.kernel_size[1]\n            )\n            \n            # Channels 0,1,2 (F1, F2, F3): use pretrained RGB weights\n            new_weights[:, 0:3, :, :] = rgb_weights\n            \n            # Channels 3,4 (S2, S3 - difference images): He initialization\n            # These need to learn from scratch as they're difference images\n            nn.init.kaiming_normal_(new_weights[:, 3:5, :, :], mode='fan_out', nonlinearity='relu')\n            \n            new_conv.weight.copy_(new_weights)\n        \n        base.features[0][0] = new_conv\n        \n        # Backbone and head\n        self.backbone = base.features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Classification head with dropout (prevent overfitting on small positive class)\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1280, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),  # Research shows dropout helps with imbalanced data\n            nn.Linear(256, 1)\n        )\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.pool(x)\n        x = self.head(x)\n        return x\n\n\n# ===================== LEARNING RATE SCHEDULE =====================\nclass WarmupCosineSchedule:\n    \"\"\"\n    Learning rate schedule with warmup + cosine decay.\n    \n    From research: Warmup prevents instability with small positive class.\n    Cosine decay smoothly reduces LR for better convergence.\n    \"\"\"\n    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-6):\n        self.optimizer = optimizer\n        self.warmup_epochs = warmup_epochs\n        self.total_epochs = total_epochs\n        self.base_lr = base_lr\n        self.min_lr = min_lr\n        self.current_epoch = 0\n    \n    def step(self):\n        if self.current_epoch < self.warmup_epochs:\n            # Linear warmup\n            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs\n        else:\n            # Cosine decay\n            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n        \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n        self.current_epoch += 1\n        return lr\n\n\n# ===================== METRICS =====================\ndef compute_metrics(y_true, y_pred, y_prob, threshold=0.5):\n    \"\"\"\n    Compute comprehensive metrics for imbalanced binary classification.\n    \n    Focus on:\n    - Recall (don't miss real asteroids)\n    - Precision (keep FP rate manageable)\n    - F1 score (balance)\n    \"\"\"\n    y_pred_binary = (y_prob >= threshold).astype(int)\n    \n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'specificity': specificity,\n        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n    }\n\n\ndef find_optimal_threshold(y_true, y_prob, min_precision=0.80):\n    \"\"\"\n    Find threshold that maximizes recall while maintaining minimum precision.\n    \n    Critical for asteroid detection: we want high recall but can't tolerate\n    too many false positives (astronomer review time is expensive).\n    \"\"\"\n    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n    \n    # Find thresholds meeting minimum precision\n    valid_idx = precisions >= min_precision\n    \n    if not valid_idx.any():\n        print(f\"⚠️  Warning: No threshold achieves precision >= {min_precision}\")\n        # Fallback: maximize F1\n        f1_scores = 2 * precisions * recalls / (precisions + recalls + 1e-10)\n        best_idx = np.argmax(f1_scores[:-1])\n        return thresholds[best_idx], precisions[best_idx], recalls[best_idx]\n    \n    # Among valid thresholds, pick one with highest recall\n    valid_recalls = recalls[:-1][valid_idx[:-1]]\n    valid_thresholds = thresholds[valid_idx[:-1]]\n    valid_precisions = precisions[:-1][valid_idx[:-1]]\n    \n    best_idx = np.argmax(valid_recalls)\n    \n    return valid_thresholds[best_idx], valid_precisions[best_idx], valid_recalls[best_idx]\n\n\n# ===================== TRAINING =====================\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_probs = []\n    all_labels = []\n    \n    pbar = tqdm(loader, desc=\"Training\")\n    for xb, yb in pbar:\n        xb, yb = xb.to(device), yb.to(device)\n        \n        optimizer.zero_grad()\n        logits = model(xb).squeeze(1)\n        loss = criterion(logits, yb.squeeze(1))\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * xb.size(0)\n        \n        with torch.no_grad():\n            probs = torch.sigmoid(logits).cpu().numpy()\n            all_probs.extend(probs)\n            all_labels.extend(yb.squeeze(1).cpu().numpy())\n        \n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    avg_loss = total_loss / len(loader.dataset)\n    metrics = compute_metrics(\n        np.array(all_labels), \n        np.array(all_probs) >= 0.5,\n        np.array(all_probs)\n    )\n    \n    return avg_loss, metrics\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device, threshold=0.5):\n    model.eval()\n    total_loss = 0\n    all_probs = []\n    all_labels = []\n    \n    for xb, yb in tqdm(loader, desc=\"Validating\"):\n        xb, yb = xb.to(device), yb.to(device)\n        \n        logits = model(xb).squeeze(1)\n        loss = criterion(logits, yb.squeeze(1))\n        \n        total_loss += loss.item() * xb.size(0)\n        \n        probs = torch.sigmoid(logits).cpu().numpy()\n        all_probs.extend(probs)\n        all_labels.extend(yb.squeeze(1).cpu().numpy())\n    \n    avg_loss = total_loss / len(loader.dataset)\n    metrics = compute_metrics(\n        np.array(all_labels),\n        np.array(all_probs) >= threshold,\n        np.array(all_probs),\n        threshold=threshold\n    )\n    \n    return avg_loss, metrics, np.array(all_probs), np.array(all_labels)\n\n\n# ===================== MAIN =====================\ndef main():\n    print(\"=\" * 60)\n    print(\"IMPROVED 5-CHANNEL ASTEROID DETECTION TRAINING\")\n    print(\"=\" * 60)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\nDevice: {device}\")\n    \n    # ===== LOAD DATA =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"LOADING DATA\")\n    print(\"=\"*60)\n    \n    train_ds = AsteroidDataset(ACTIVE_CSV, \"train\", DATA_DIR, augment=True)\n    val_ds = AsteroidDataset(ACTIVE_CSV, \"val\", DATA_DIR, augment=False)\n    test_ds = AsteroidDataset(ACTIVE_CSV, \"test\", DATA_DIR, augment=False)\n    \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n                             num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n                           num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n                            num_workers=2, pin_memory=True)\n    \n    # ===== MODEL =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"INITIALIZING MODEL\")\n    print(\"=\"*60)\n    \n    model = EfficientNet5Channel().to(device)\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # ===== OPTIMIZER & LOSS =====\n    criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n    scheduler = WarmupCosineSchedule(optimizer, WARMUP_EPOCHS, NUM_EPOCHS, BASE_LR)\n    \n    print(f\"\\nLoss: Focal Loss (alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA})\")\n    print(f\"Optimizer: AdamW (lr={BASE_LR}, wd={WEIGHT_DECAY})\")\n    print(f\"Scheduler: Warmup({WARMUP_EPOCHS}) + Cosine\")\n    \n    # ===== TRAINING LOOP =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING\")\n    print(\"=\"*60)\n    \n    best_recall = 0\n    best_f1 = 0\n    patience_counter = 0\n    history = {'train_loss': [], 'val_loss': [], 'val_recall': [], 'val_precision': [], 'val_f1': []}\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n        print(\"-\" * 60)\n        \n        # Learning rate step\n        lr = scheduler.step()\n        print(f\"Learning rate: {lr:.6f}\")\n        \n        # Train\n        train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Train Loss: {train_loss:.4f} | \"\n              f\"Prec: {train_metrics['precision']:.3f} | \"\n              f\"Rec: {train_metrics['recall']:.3f} | \"\n              f\"F1: {train_metrics['f1']:.3f}\")\n        \n        # Validate\n        val_loss, val_metrics, val_probs, val_labels = validate(\n            model, val_loader, criterion, device\n        )\n        print(f\"Val Loss:   {val_loss:.4f} | \"\n              f\"Prec: {val_metrics['precision']:.3f} | \"\n              f\"Rec: {val_metrics['recall']:.3f} | \"\n              f\"F1: {val_metrics['f1']:.3f}\")\n        print(f\"Val Confusion: TP={val_metrics['tp']}, FP={val_metrics['fp']}, \"\n              f\"TN={val_metrics['tn']}, FN={val_metrics['fn']}\")\n        \n        # Save history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_recall'].append(val_metrics['recall'])\n        history['val_precision'].append(val_metrics['precision'])\n        history['val_f1'].append(val_metrics['f1'])\n        \n        # Save best model (prioritize recall, then F1)\n        if val_metrics['recall'] > best_recall or \\\n           (val_metrics['recall'] == best_recall and val_metrics['f1'] > best_f1):\n            best_recall = val_metrics['recall']\n            best_f1 = val_metrics['f1']\n            torch.save(model.state_dict(), WORK_DIR / \"best_model.pt\")\n            print(f\"✓ Saved best model (Recall: {best_recall:.3f}, F1: {best_f1:.3f})\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            print(f\"Patience: {patience_counter}/{PATIENCE}\")\n        \n        # Early stopping\n        if patience_counter >= PATIENCE:\n            print(f\"\\n⚠️  Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    # ===== FINAL EVALUATION =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL EVALUATION ON TEST SET\")\n    print(\"=\"*60)\n    \n    # Load best model\n    model.load_state_dict(torch.load(WORK_DIR / \"best_model.pt\"))\n    \n    # Get predictions\n    _, test_metrics_default, test_probs, test_labels = validate(\n        model, test_loader, criterion, device, threshold=0.5\n    )\n    \n    print(\"\\n--- Metrics with default threshold (0.5) ---\")\n    print(f\"Precision: {test_metrics_default['precision']:.3f}\")\n    print(f\"Recall:    {test_metrics_default['recall']:.3f}\")\n    print(f\"F1 Score:  {test_metrics_default['f1']:.3f}\")\n    print(f\"Confusion Matrix:\")\n    print(f\"  TP={test_metrics_default['tp']}, FP={test_metrics_default['fp']}\")\n    print(f\"  FN={test_metrics_default['fn']}, TN={test_metrics_default['tn']}\")\n    \n    # Find optimal threshold\n    opt_thresh, opt_prec, opt_rec = find_optimal_threshold(\n        test_labels, test_probs, min_precision=MIN_PRECISION\n    )\n    \n    print(f\"\\n--- Optimal threshold (min precision={MIN_PRECISION}) ---\")\n    print(f\"Threshold: {opt_thresh:.4f}\")\n    print(f\"Precision: {opt_prec:.3f}\")\n    print(f\"Recall:    {opt_rec:.3f}\")\n    \n    test_metrics_opt = compute_metrics(\n        test_labels, test_probs >= opt_thresh, test_probs, threshold=opt_thresh\n    )\n    print(f\"F1 Score:  {test_metrics_opt['f1']:.3f}\")\n    print(f\"Confusion Matrix:\")\n    print(f\"  TP={test_metrics_opt['tp']}, FP={test_metrics_opt['fp']}\")\n    print(f\"  FN={test_metrics_opt['fn']}, TN={test_metrics_opt['tn']}\")\n    \n    # Save results\n    results = {\n        'test_metrics_default': test_metrics_default,\n        'test_metrics_optimal': test_metrics_opt,\n        'optimal_threshold': opt_thresh,\n        'history': history\n    }\n    \n        # Save full results (including predictions for later visualization)\n    results = {\n        'test_metrics_default': test_metrics_default,\n        'test_metrics_optimal': test_metrics_opt,\n        'optimal_threshold': opt_thresh,\n        'history': history,\n        'test_labels': test_labels,\n        'test_probs': test_probs\n    }\n\n    torch.save(results, WORK_DIR / \"training_results.pt\")\n    print(f\"✓ Saved results with predictions: {WORK_DIR / 'training_results.pt'}\")\n\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\"*60)\n    print(f\"Best model saved: {WORK_DIR / 'best_model.pt'}\")\n    print(f\"Results saved: {WORK_DIR / 'training_results.pt'}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nVisualize TP / FP / FN / TN samples after model training\n=========================================================\nLoads saved test predictions and metadata,\nthen shows 1×5 channel strips for each classification category.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nDATA_DIR = Path(\"/kaggle/input/stamps-clean/stamps_clean\")\nMETA_PATH = DATA_DIR / \"metadata.csv\"\n\ndef visualize_classification_examples(metadata, y_true, y_prob, threshold, n_samples=6):\n    \"\"\"\n    Visualize TP, FP, FN, TN examples as 1×5 channel strips.\n    Each row shows [F₁–S₃] with per-row probability label (p̂).\n    Layout is compact with minimal gap between label and image.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    y_pred = (y_prob >= threshold).astype(int)\n\n    # Indices for each outcome type\n    tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n    fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n    fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n    tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n\n    print(f\"TP={len(tp_idx)}, FP={len(fp_idx)}, FN={len(fn_idx)}, TN={len(tn_idx)}\")\n\n    channel_labels = [\"F₁\", \"F₂\", \"F₃\", \"S₂\", \"S₃\"]\n\n    def plot_subset(indices, title):\n        if len(indices) == 0:\n            print(f\"No {title} examples found.\")\n            return\n\n        idxs = np.random.choice(indices, min(n_samples, len(indices)), replace=False)\n        fig, axes = plt.subplots(len(idxs), 5, figsize=(12, 2.2 * len(idxs)))\n        fig.suptitle(title, fontsize=13, y=0.99)\n\n        if len(idxs) == 1:\n            axes = np.expand_dims(axes, axis=0)\n\n        for i, idx in enumerate(idxs):\n            row = metadata.iloc[idx]\n            arr = np.load(DATA_DIR / row[\"stamp_path\"])[\"x\"]\n\n            # Draw each channel with top labels\n            for j in range(5):\n                ax = axes[i, j]\n                ax.imshow(arr[j], cmap=\"gray\", vmin=-3, vmax=3)\n                ax.set_title(channel_labels[j], fontsize=9, pad=2)\n                ax.axis(\"off\")\n\n            # Probability label (closer to image)\n            prob_text = f\"p̂ = {y_prob[idx]:.3f}\"\n            axes[i, 0].text(\n                -0.25, 0.5, prob_text,   # 👈 tighter spacing (was -0.6)\n                va=\"center\", ha=\"right\",\n                fontsize=9, color=\"black\",\n                transform=axes[i, 0].transAxes\n            )\n\n        plt.subplots_adjust(\n            left=0.05, right=0.98, top=0.94, bottom=0.05,\n            wspace=0.05, hspace=0.15\n        )\n        plt.show()\n\n    # Plot each outcome group\n    plot_subset(tp_idx, \"True Positives (Correct Asteroids)\")\n    plot_subset(fp_idx, \"False Positives (Non-Asteroids Misclassified)\")\n    plot_subset(fn_idx, \"False Negatives (Missed Asteroids)\")\n    plot_subset(tn_idx, \"True Negatives (Correct Non-Asteroids)\")\n\ndef main():\n    print(\"=\"*60)\n    print(\"VISUALIZING TEST CLASSIFICATIONS\")\n    print(\"=\"*60)\n\n    # Load results with predictions\n    results_path = WORK_DIR / \"training_results.pt\"\n    results = torch.load(results_path, weights_only=False)\n    y_true = np.array(results[\"test_labels\"])\n    y_prob = np.array(results[\"test_probs\"])\n    threshold = results[\"optimal_threshold\"]\n\n    # Load metadata\n    metadata = pd.read_csv(META_PATH)\n    metadata = metadata[metadata[\"split\"] == \"test\"].reset_index(drop=True)\n\n    visualize_classification_examples(metadata, y_true, y_prob, threshold)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_classification_examples(metadata, y_true, y_prob, optimal_thresh, n_samples=6):\n    \"\"\"\n    Visualize TP, FP, FN, TN examples from test dataset.\n    Each example shows a 1×5 strip of the five channels [F1, F2, F3, S2, S3].\n    \"\"\"\n    import random\n\n    y_pred = (y_prob >= optimal_thresh).astype(int)\n\n    # Identify indices\n    tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n    fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n    fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n    tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n\n    print(f\"\\nTP={len(tp_idx)}, FP={len(fp_idx)}, FN={len(fn_idx)}, TN={len(tn_idx)}\")\n\n    def show_examples(idxs, title):\n        if len(idxs) == 0:\n            print(f\"No {title.lower()} examples available.\")\n            return\n        subset = random.sample(list(idxs), min(n_samples, len(idxs)))\n        fig, axes = plt.subplots(len(subset), 5, figsize=(15, 3*len(subset)))\n        fig.suptitle(title, fontsize=14)\n        for i, idx in enumerate(subset):\n            row = metadata.iloc[idx]\n            path = DATA_DIR / row['stamp_path']\n            arr = np.load(path)['x']  # shape (5, H, W)\n            for j in range(5):\n                ax = axes[i, j] if len(subset) > 1 else axes[j]\n                ax.imshow(arr[j], cmap='gray', vmin=-3, vmax=3)\n                if i == 0:\n                    ax.set_title(['F1','F2','F3','S2','S3'][j])\n                ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n    # Show examples per class\n    show_examples(tp_idx, \"True Positives (Correct Asteroid Detections)\")\n    show_examples(fp_idx, \"False Positives (Non-Asteroids Classified as Asteroids)\")\n    show_examples(fn_idx, \"False Negatives (Missed Asteroids)\")\n    show_examples(tn_idx, \"True Negatives (Correct Non-Asteroids)\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # === Visualize TP, FP, FN, TN samples ===\n    print(\"\\n\" + \"=\"*60)\n    print(\"VISUALIZING SAMPLE CLASSIFICATIONS\")\n    print(\"=\"*60)\n\n    # Load predictions if saved, else skip\n    if \"test_labels\" in results and \"test_probs\" in results:\n        y_true = np.array(results[\"test_labels\"])\n        y_prob = np.array(results[\"test_probs\"])\n        visualize_classification_examples(metadata, y_true, y_prob, optimal_thresh)\n    else:\n        print(\"⚠️ Skipping visualization: test predictions not saved in results.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}